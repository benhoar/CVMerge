{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [10:53:46<00:00,  2.04it/s]  \n"
     ]
    }
   ],
   "source": [
    "# PRIMARY MERGE CV CODE\n",
    "\n",
    "import warnings, datetime, pandas as pd, os, numpy as np, random as r, pickle, math\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "class DataPretreatment:\n",
    "   def __init__(\n",
    "                  self, \n",
    "                  forward_current_limit=0.05, \n",
    "                  reverse_current_limit=0.05,\n",
    "                  root_directory='.',\n",
    "                  data_folder='Samples',\n",
    "                  mechs=['ECa', 'ECb', 'ECE', 'DISP', 'ECP', 'T', 'E', 'SR'],\n",
    "                  dictating_scan_rate=-1\n",
    "   ):\n",
    "      self.forward_current_limit = forward_current_limit\n",
    "      self.reverse_current_limit = reverse_current_limit\n",
    "      self.root_directory = root_directory \n",
    "      self.data_folder = data_folder\n",
    "      self.mechs = mechs\n",
    "      self.dictating_scan_rate=dictating_scan_rate\n",
    "\n",
    "   def label_merge_safety(self):\n",
    "      path = f'{self.root_directory}/Merge_Safety'\n",
    "      if not os.path.isdir(path):\n",
    "         os.mkdir(path)\n",
    "      data_path = './Samples'\n",
    "      for mech in self.mechs: \n",
    "         files = os.listdir(f'{self.root_directory}/{self.data_folder}/{mech}')\n",
    "         files = [f for f in files if f.endswith('.txt')]\n",
    "         merge_safe = {}\n",
    "         conditional_safe = {}\n",
    "         if mech == 'SR':\n",
    "            merge_safe = {k:(True, True) for k in files}\n",
    "         elif mech == 'ECP' or mech == 'T':\n",
    "            conditional_safe = {k:(True, False) for k in files}\n",
    "         else:\n",
    "            print(f'Treating: {mech}')\n",
    "            for file in tqdm(files):\n",
    "               try:\n",
    "                  df = pd.read_csv(f'{data_path}/{mech}/{file}')\n",
    "                  srs = df['v'].unique()\n",
    "                  df = df.loc[df['v'] == srs[self.dictating_scan_rate]]\n",
    "                  n = len(df)//2\n",
    "                  fwd = df.iloc[:n, :]\n",
    "                  rev = df.iloc[n:, :]\n",
    "                  d_fwd = abs(fwd.loc[fwd.index[-50], 'A'] - fwd.loc[fwd.index[-1], 'A'])\n",
    "                  d_rev = abs(rev.loc[rev.index[-50], 'A'] - rev.loc[rev.index[-1], 'A'])\n",
    "                  fwd_is_decreasing = fwd.loc[fwd.index[-50], 'A'] >= fwd.loc[fwd.index[-49], 'A']\n",
    "                  rev_is_increasing = rev.loc[rev.index[-50], 'A'] <= rev.loc[rev.index[-49], 'A']\n",
    "                  left_merge = d_rev <= self.reverse_current_limit #and rev_is_increasing\n",
    "                  right_merge = d_fwd <= self.forward_current_limit #and fwd_is_decreasing\n",
    "                  if not fwd_is_decreasing or not rev_is_increasing:\n",
    "                     continue\n",
    "                  if left_merge and right_merge:\n",
    "                     merge_safe[file] = (left_merge, right_merge)\n",
    "                  elif left_merge or right_merge:\n",
    "                     conditional_safe[file] = (left_merge, right_merge)\n",
    "               except:\n",
    "                  print(file)\n",
    "         pickle.dump(merge_safe, open(f'{path}/{mech}_safe.pkl','wb'))\n",
    "         pickle.dump(conditional_safe, open(f'{path}/{mech}_half_safe.pkl','wb'))\n",
    "\n",
    "   def gen_scan_rate_data(self):\n",
    "      path = f'{self.root_directory}/Scan_Rate_Info'\n",
    "      if not os.path.isdir(path):\n",
    "         os.mkdir(path)\n",
    "      for mech in self.mechs:\n",
    "         file_path = f'{self.root_directory}/{self.data_folder}/{mech}'\n",
    "         files = os.listdir(file_path)\n",
    "         files = [f for f in files if f.endswith('txt')]\n",
    "         data = []\n",
    "         print(mech)\n",
    "         for file in tqdm(files):\n",
    "            try:\n",
    "               df = pd.read_csv(f'{file_path}/{file}')\n",
    "               srs = df['v'].unique()\n",
    "               max_sr = math.log10(max(srs))\n",
    "               min_sr = math.log10(min(srs))\n",
    "               sr_range = max_sr-min_sr\n",
    "               data.append([file, sr_range])\n",
    "            except:\n",
    "               print(file)\n",
    "         res = pd.DataFrame(data, columns=[\"File\", \"Range\"])\n",
    "         res.to_csv(f'{path}/{mech}_SRs.csv', index=False)\n",
    "      print(\"Done Generating Scan Rate Sheets\")\n",
    "\n",
    "   def clean_up(self):\n",
    "      global_min = float('-inf')\n",
    "      global_max = float('inf')\n",
    "      sr_path = f'{self.root_directory}/Scan_Rate_Info'\n",
    "      for file in os.listdir(sr_path):\n",
    "         data = pd.read_csv(f'{sr_path}/{file}')\n",
    "         global_min = max(global_min, min(data['Range']))\n",
    "         global_max = min(global_max, max(data['Range']))\n",
    "      \n",
    "      merge_path = f'{self.root_directory}/Merge_Safety'\n",
    "      for file in os.listdir(sr_path):\n",
    "         mech = file[:file.find('_')]\n",
    "         data = pd.read_csv(f'{sr_path}/{file}')\n",
    "         data = data.loc[(data['Range'] >= global_min) & (data['Range'] <= global_max)]\n",
    "         data.to_csv(f'{sr_path}/{file}', index=False)\n",
    "         half_safe = pickle.load(open(f'{merge_path}/{mech}_half_safe.pkl', 'rb'))\n",
    "         safe = pickle.load(open(f'{merge_path}/{mech}_safe.pkl', 'rb'))\n",
    "         half_safe = {k:v for k,v in half_safe.items() if data.loc[:,'File'].str.contains(k).any()}\n",
    "         safe = {k:v for k,v in safe.items() if data.loc[:,'File'].str.contains(k).any()}\n",
    "         pickle.dump(half_safe, open(f'{merge_path}/{mech}_half_safe.pkl', 'wb'))\n",
    "         pickle.dump(safe, open(f'{merge_path}/{mech}_safe.pkl', 'wb'))\n",
    "             \n",
    "class Mech:\n",
    "   def __init__(self, mech, cathodic, scale_min=0.2):\n",
    "      self.mech = mech\n",
    "      self.cathodic = cathodic\n",
    "      self.data = None\n",
    "      self.file = None\n",
    "      self.scaling = r.randint(100*scale_min, 100)/100\n",
    "      self.bounding_box = []\n",
    "      self.reversible = True\n",
    "\n",
    "   def __str__(self):\n",
    "      return f'Mech: {self.mech}, Cathodic: {self.cathodic}, Scaling: {self.scaling}, Reversible: {self.reversible}, File: {self.file}'\n",
    "\n",
    "class MergeCV:\n",
    "   def __init__(\n",
    "                  self, \n",
    "                  date,\n",
    "                  num_to_merge=4,\n",
    "                  scale_min=0.2, \n",
    "                  fractional_peak_current=0.25,\n",
    "                  fractional_SR_current=0.1,\n",
    "                  fractional_T_current=0.01,\n",
    "                  dictating_scan_rate=-1,\n",
    "                  peak_ratio=0.1,\n",
    "                  additional_shift_min=0,\n",
    "                  additional_shift_max=1000,\n",
    "                  scan_range_tolerance=0.2,\n",
    "                  exclude=[]         \n",
    "   ):\n",
    "      self.date=date\n",
    "      self.num_to_merge = num_to_merge\n",
    "      self.scale_min = scale_min\n",
    "      self.fractional_peak_current = fractional_peak_current\n",
    "      self.fractional_SR_current = fractional_SR_current\n",
    "      self.fractional_T_current = fractional_T_current\n",
    "      self.dictating_scan_rate = dictating_scan_rate\n",
    "      self.peak_ratio = peak_ratio\n",
    "      self.additional_shift_min = additional_shift_min\n",
    "      self.additional_shift_max = additional_shift_max\n",
    "      self.name = \"\"\n",
    "      self.mechs = []\n",
    "      self.res = None\n",
    "      self.final_srs = []\n",
    "      self.scan_range_tolerance = scan_range_tolerance\n",
    "      self.exclude = exclude\n",
    "\n",
    "   def __get_mechs(self):\n",
    "      mechs = [None]*self.num_to_merge\n",
    "      x = r.randint(1, 3)\n",
    "      if x == 1:\n",
    "         cath = round(r.random())\n",
    "         if cath:\n",
    "            mechs[0] = Mech(\"T\", True, self.scale_min)\n",
    "         else:\n",
    "            mechs[-1] = Mech(\"T\", False, self.scale_min)\n",
    "\n",
    "      x = r.randint(1, 4)\n",
    "      if x == 2:\n",
    "         cath = round(r.random())\n",
    "         if cath and not mechs[0]:\n",
    "            mechs[0] = Mech(\"ECP\", True, self.scale_min)\n",
    "         elif cath:\n",
    "            mechs[-1] = Mech(\"ECP\", False, self.scale_min)\n",
    "         elif mechs[-1]:\n",
    "            mechs[0] = Mech(\"ECP\", True, self.scale_min)\n",
    "         else:\n",
    "            mechs[-1] = Mech(\"ECP\", False, self.scale_min)\n",
    "      # HERE UPDATE MECHS\n",
    "      remaining_mechs = [\"E\", \"ECa\", \"ECb\", \"SR\", \"ECE\", \"DISP\"]\n",
    "      remaining_mechs = [m for m in remaining_mechs if m not in self.exclude]\n",
    "      for i, spot in enumerate(mechs):\n",
    "         if not spot:\n",
    "            x = r.randint(0, len(remaining_mechs)-1)\n",
    "            cathodic = True if round(r.random()) == 1 else False\n",
    "            mechs[i] = Mech(remaining_mechs[x], cathodic, self.scale_min)\n",
    "      self.mechs = mechs\n",
    "   \n",
    "   # find file with valid range of scan rates\n",
    "   def __get_valid_file(self, options, mechanism, host_range):\n",
    "      if host_range == 0:\n",
    "         return r.choice(options)\n",
    "      sr_info = pd.read_csv(f'./Scan_Rate_Info/{mechanism}_SRs.csv')\n",
    "      min_sr_range = host_range-self.scan_range_tolerance\n",
    "      max_sr_range = host_range+self.scan_range_tolerance\n",
    "      sr_info = sr_info.loc[(sr_info['Range'] >= min_sr_range) & (sr_info['Range'] <= max_sr_range)]\n",
    "      sr_info = sr_info.loc[sr_info['File'].isin(options)]\n",
    "      indices = sr_info.index\n",
    "      return sr_info.loc[r.choice(indices), 'File']  \n",
    "\n",
    "   # labelling may have to be flipped with ECa and ECb cases\n",
    "   def __check_labelling(self, mech, file):\n",
    "      file_mech = file[:file.find('_')]\n",
    "      if file_mech == 'EC' and mech.cathodic or file_mech == 'CE' and not mech.cathodic:\n",
    "         mech.mech = 'ECb'\n",
    "      if file_mech == 'CE' and mech.cathodic or file_mech == 'EC' and not mech.cathodic:\n",
    "         mech.mech = 'ECa'\n",
    "\n",
    "   # get files with appropriate merge-safety\n",
    "   def __get_files(self):\n",
    "      host_range = 0\n",
    "      n = self.num_to_merge\n",
    "      for i, mech in enumerate(self.mechs):\n",
    "         # safe means it can be merged from either side, so no special consideration is necessary\n",
    "         safe = pickle.load(open(f'./Merge_Safety/{mech.mech}_safe.pkl', 'rb'))\n",
    "         # half safe means it is useable as a beginning or terminal mechanism so needs to be looked at\n",
    "         half_safe = pickle.load(open(f'./Merge_Safety/{mech.mech}_half_safe.pkl', 'rb'))\n",
    "\n",
    "         options = None\n",
    "         if n <= 2 or i == 0 or i == n-1:\n",
    "            options = list(safe.keys())\n",
    "            options.extend(list(half_safe.keys()))\n",
    "         else:\n",
    "            options = list(safe.keys())\n",
    "         file = self.__get_valid_file(options, mech.mech, host_range)\n",
    "         if file not in safe:\n",
    "            left_safe = half_safe[file][0]\n",
    "            right_safe = half_safe[file][1]\n",
    "            if i == 0 and left_safe or i == n-1 and right_safe:\n",
    "               mech.cathodic = True\n",
    "            if i == 0 and right_safe or i == n-1 and left_safe:\n",
    "               mech.cathodic = False\n",
    "         if mech.mech == 'ECb' or mech.mech == 'ECa':\n",
    "            self.__check_labelling(mech, file)\n",
    "         \n",
    "         mech.file = file\n",
    "         self.name += file[:-4] + \"_\"\n",
    "         mech.data = pd.read_csv(f'./Samples/{mech.mech}/{file}')\n",
    "         if host_range == 0:\n",
    "            host_range = math.log10(max(mech.data['v'])) - math.log10(min(mech.data['v']))\n",
    "         min_V = min(mech.data['V'])\n",
    "         mech.data.loc[:,'V'] -= min_V\n",
    "         mech.data.reset_index(inplace=True, drop=True)\n",
    "      now = datetime.datetime.now().time()\n",
    "      id = now.strftime(\"%H%M%S\") + str(r.randint(1, 1000))\n",
    "      self.name += id + \"_\"\n",
    "\n",
    "   # ensure forward and reverse scan have same number of entries\n",
    "   def __mirrorify(self, mech):\n",
    "      scan_rates = mech.data['v'].unique()\n",
    "      data = mech.data[mech.data['v'] == scan_rates[self.dictating_scan_rate]].copy()\n",
    "      n = data.loc[data['V'] == max(data['V'])].index[0]\n",
    "      fwd_df = data.loc[data.index <= n].copy()\n",
    "      rev_df = data.loc[data.index > n].copy()\n",
    "      pots = sorted(pd.Series(list(set(fwd_df['V']).intersection(set(rev_df['V'])))))\n",
    "      # cutoff of first and last points due to COMSOL phenomenon\n",
    "      pots = pots[1:-1]\n",
    "      mech.data = mech.data.loc[mech.data['V'].isin(pots)]\n",
    "\n",
    "   def __adjust_data(self, mech):\n",
    "      flip = -1 if mech.cathodic else 1\n",
    "      mech.data.loc[:, 'A'] *= mech.scaling*flip\n",
    "      res = pd.DataFrame()\n",
    "      if mech.cathodic:\n",
    "         for v in mech.data['v'].unique():\n",
    "            t = mech.data.loc[mech.data['v'] == v].copy()\n",
    "            og_pots = list(t['V'])\n",
    "            n = t.index[len(t)//2]\n",
    "            fwd = t.loc[t.index < n]\n",
    "            rev = t.loc[t.index >= n]\n",
    "            rev = pd.concat([rev, fwd])\n",
    "            rev.loc[:, 'V'] = og_pots\n",
    "            res = pd.concat([res, rev])\n",
    "         mech.data = res\n",
    "      mech.data.reset_index(inplace=True, drop=True) \n",
    "\n",
    "   def __get_reversibility(self, mech):\n",
    "      if mech.mech == 'T' or mech.mech == 'ECP':\n",
    "         return False\n",
    "      scan_rates = mech.data['v'].unique()\n",
    "      lcl_data = mech.data.loc[mech.data['v'] == scan_rates[self.dictating_scan_rate]].copy()\n",
    "      min_cur = min(lcl_data['A'])\n",
    "      max_cur = max(lcl_data['A'])\n",
    "      peak_ratio = abs(min_cur/max_cur) if abs(min_cur) < max_cur else abs(max_cur/min_cur)\n",
    "      last_cur = lcl_data.loc[lcl_data.index[-1], 'A']\n",
    "      if abs(last_cur-min_cur) < 0.02:\n",
    "         return False\n",
    "      return peak_ratio >= 0.1\n",
    "   \n",
    "   def __find_index(self, l, r, target, data):\n",
    "      decreasing = data.loc[l, 'A'] > data.loc[r, 'A']\n",
    "      while (l <= r):\n",
    "         m = l+(r-l) // 2\n",
    "         if data.loc[m, 'A'] < target:\n",
    "            if decreasing:\n",
    "               r = m-1\n",
    "            else:\n",
    "               l = m+1\n",
    "         else:\n",
    "            if decreasing:\n",
    "               l = m+1\n",
    "            else:\n",
    "               r = m-1\n",
    "      return l if l <= data.index[-1] else data.index[-1]\n",
    "\n",
    "   def __handle_tafel(self, mech, data):\n",
    "      indices = data.index\n",
    "      min_cur = data.loc[indices[0], 'A']\n",
    "      max_cur = max(data['A'])\n",
    "      target = self.fractional_T_current*(max_cur-min_cur)+min_cur \n",
    "      n = len(indices)//2\n",
    "      index = self.__find_index(indices[0], indices[n-1], target, data)\n",
    "      mech.data['merge_point'] = False\n",
    "      mech.data.loc[index, 'merge_point'] = True\n",
    "      mech.data.loc[indices[n-1], 'merge_point'] = True\n",
    "\n",
    "   def __handle_ECP(self, data):\n",
    "      indices = data.index\n",
    "      min_cur = data.loc[indices[0], 'A']\n",
    "      max_cur = max(data['A'])\n",
    "      target = self.fractional_T_current*(max_cur-min_cur)+min_cur \n",
    "      n = len(indices)//2\n",
    "      index = self.__find_index(indices[0], indices[n-1], target, data)\n",
    "      return index\n",
    "  \n",
    "   # find minimum legal merge points/bounding interval\n",
    "   def __calculate_merge_reps(self, mech):\n",
    "      scan_rates = mech.data['v'].unique()\n",
    "      rep_data = mech.data[mech.data['v'] == scan_rates[self.dictating_scan_rate]].copy()\n",
    "\n",
    "      if mech.mech == 'T':\n",
    "         self.__handle_tafel(mech, rep_data)\n",
    "         return\n",
    "\n",
    "      max_A = max(rep_data['A'])\n",
    "      min_A = min(rep_data['A'])\n",
    "      \n",
    "      main_peak = max_A if max_A > abs(min_A) else min_A\n",
    "      # Mpi = max peak index, mpi = min peak index\n",
    "      Mpi = rep_data[rep_data['A'] == max_A].index[0]\n",
    "      mpi = rep_data[rep_data['A'] == min_A].index[0]\n",
    "\n",
    "      merge_points = []\n",
    "      indices = rep_data.index\n",
    "      l = indices[0]\n",
    "      r = indices[-1]\n",
    "      n = indices[(len(indices)//2)-1]\n",
    "      frac = self.fractional_peak_current\n",
    "      if mech.mech == 'SR':\n",
    "         frac = self.fractional_SR_current\n",
    "\n",
    "      if rep_data.loc[Mpi, 'A'] == main_peak or mech.reversible:\n",
    "         target = frac*(max_A-rep_data.loc[n,'A']) + rep_data.loc[n,'A']\n",
    "         merge_points.append(self.__find_index(Mpi, n, target, rep_data))\n",
    "         if not mech.reversible and not mech.mech == 'ECP':\n",
    "            merge_points.append(max(l, Mpi-abs((merge_points[-1]-Mpi))))\n",
    "         if mech.mech == 'ECP':\n",
    "            merge_points.append(self.__handle_ECP(rep_data))\n",
    "\n",
    "      if rep_data.loc[mpi, 'A'] == main_peak or mech.reversible:\n",
    "         target = rep_data.loc[r,'A'] - frac*(abs(min_A-rep_data.loc[r,'A']))\n",
    "         merge_points.append(self.__find_index(mpi, r, target, rep_data))\n",
    "         if not mech.reversible:\n",
    "            merge_points.append(min(n, mpi-abs((merge_points[-1]-mpi))))\n",
    "   \n",
    "      mech.data['merge_point'] = False\n",
    "      for point in merge_points:\n",
    "         mech.data.loc[point, 'merge_point'] = True\n",
    "\n",
    "   def initialize_data(self):\n",
    "      self.__get_mechs()\n",
    "      self.__get_files()\n",
    "      for mech in self.mechs:\n",
    "         if not self.final_srs:\n",
    "            self.final_srs = list(mech.data['v'].unique())\n",
    "         self.__mirrorify(mech)\n",
    "         mech.reversible = self.__get_reversibility(mech)\n",
    "         self.__calculate_merge_reps(mech)\n",
    "         self.__adjust_data(mech)\n",
    "\n",
    "   def run_spoof(self, files, orientations, scales):\n",
    "      for i, mech in enumerate(self.mechs):\n",
    "         file = files[i]\n",
    "         scale = scales[i]\n",
    "         orientation = orientations[i]\n",
    "         mech.mech = file[:file.find(\"_\")]\n",
    "         mech.file = file\n",
    "         mech.scaling = scale\n",
    "         mech.direction = orientation\n",
    "         mech.data = pd.read_csv(f'./Samples/{mech.mech}/{file}.txt')\n",
    "         mech.reversible = self.__get_reversibility(mech)\n",
    "         self.__adjust_data(mech)\n",
    "         self.__calculate_merge_reps(mech)\n",
    "         \n",
    "   def __calculate_shifts(self):\n",
    "      def get_mps(indices, data):\n",
    "         max_potential = data.loc[indices[0], 'V']\n",
    "         min_potential = max_potential\n",
    "         for i in range(1, len(indices)):\n",
    "            cur = data.loc[indices[i], 'V']\n",
    "            max_potential = max(max_potential, cur)\n",
    "            min_potential = min(min_potential, cur)\n",
    "         return [min_potential, max_potential]\n",
    "\n",
    "      host = self.mechs[0].data\n",
    "      mpis = host.loc[host['merge_point']].index\n",
    "      host_mps = get_mps(mpis, host)\n",
    "      shifts = [0]\n",
    "      for i in range(1, len(self.mechs)):\n",
    "         guest = self.mechs[i].data\n",
    "         mpis = guest.loc[guest['merge_point']].index\n",
    "         guest_mps = get_mps(mpis, guest)\n",
    "         host_pot = host_mps[1]\n",
    "         guest_pot = guest_mps[0]\n",
    "         rand_shift = r.randint(self.additional_shift_min, self.additional_shift_max)\n",
    "         shift = (host_pot-guest_pot) + rand_shift\n",
    "         shifts.append(shift)\n",
    "         guest_mps[1] += shift\n",
    "         host_mps = guest_mps\n",
    "      return shifts\n",
    "\n",
    "   def __calculate_bounding(self, mech):\n",
    "      mpis = mech.data.loc[mech.data['merge_point']].index\n",
    "      mps = [mech.data.loc[i, 'V'] for i in mpis]\n",
    "      min_mp = min(mps)\n",
    "      max_mp = max(mps)\n",
    "      mech.bounding_box = [min_mp, max_mp]\n",
    "\n",
    "   def format_data(self):\n",
    "      shifts = self.__calculate_shifts()\n",
    "      for i, mech in enumerate(self.mechs):\n",
    "         mech.data.loc[:,'V'] += shifts[i]\n",
    "         mapping = {v:i for i,v in enumerate(mech.data['v'].unique())}\n",
    "         mech.data.replace({'v': mapping}, inplace=True)\n",
    "         self.__calculate_bounding(mech)\n",
    "         mech.data.drop(['merge_point'], axis=1, inplace=True)\n",
    "\n",
    "   def __merge_dfs(self, dfs, sr, rev=False):\n",
    "      res = pd.DataFrame()\n",
    "      left_adds = []\n",
    "      right_adds = []\n",
    "      max_V = max(dfs[-1]['V'])\n",
    "      min_V = min(dfs[0]['V'])\n",
    "      res = pd.DataFrame(columns=['V','A','v'])\n",
    "      res['V'] = range(int(min_V), int(max_V))\n",
    "      res['v'] = sr\n",
    "      for df in dfs:\n",
    "         if rev:\n",
    "            df = df[::-1]\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "         left_adds.append([min(df['V']), df.loc[df.index[0], 'A']])\n",
    "         right_adds.append([max(df['V']), df.loc[df.index[-1], 'A']])   \n",
    "         res = pd.concat([res, df]).groupby(['V','v']).sum().reset_index()\n",
    "      for left in left_adds:\n",
    "         res.loc[res['V'] < left[0], 'A'] += left[1]\n",
    "      for right in right_adds:\n",
    "         res.loc[res['V'] > right[0], 'A'] += right[1]\n",
    "      return res\n",
    "\n",
    "   def merge(self, single=False):\n",
    "      res = pd.DataFrame()\n",
    "      for i in range(6):\n",
    "         fwd_dfs = []\n",
    "         rev_dfs = []\n",
    "         name = \"\"\n",
    "         global_max = 0\n",
    "         for j, mech in enumerate(self.mechs):\n",
    "            if j == self.num_to_merge-1:\n",
    "               global_max = max(mech.data['V'])\n",
    "            name += mech.file[:-4]\n",
    "            data = mech.data[mech.data['v'] == i].copy()\n",
    "            data.reset_index(inplace=True, drop=True)\n",
    "            indices = data.index\n",
    "            n = indices[len(indices)//2]\n",
    "            fwd_df = data.iloc[:n, :].copy()\n",
    "            rev_df = data.iloc[n:, :].copy()\n",
    "            fwd_dfs.append(fwd_df)\n",
    "            rev_dfs.append(rev_df)\n",
    "\n",
    "         fwd_full = self.__merge_dfs(fwd_dfs, i)\n",
    "         rev_full = self.__merge_dfs(rev_dfs, i, rev=True)\n",
    "         rev_full = rev_full[::-1]\n",
    "         res = pd.concat([res, fwd_full, rev_full])\n",
    "         if single:\n",
    "            res = res.loc[(res['V'] > 0) & (res['V'] < global_max)]\n",
    "            res.reset_index(inplace=True, drop=True)\n",
    "            self.res = res\n",
    "            return res # tabbed in for single scan rate\n",
    "      res = res.loc[(res['V'] > 0) & (res['V'] < global_max)]\n",
    "      mapping = {i:v for i,v in enumerate(self.final_srs)}\n",
    "      res.reset_index(inplace=True, drop=True)\n",
    "      self.res = res\n",
    "      self.res.replace({'v': mapping}, inplace=True)\n",
    "      return res\n",
    "\n",
    "   def simple_plot(self, mps=None):\n",
    "      for mech in self.mechs:\n",
    "         data = mech.data\n",
    "         plt.scatter(data['V'], data['A'], s=0.1, c=data['v'], cmap='Set2')\n",
    "         if mps:\n",
    "            plt.bar(min(mps), abs(max(data['A'])-min(data['A'])), bottom=min(data['A']), width=20, color='k')\n",
    "            plt.bar(max(mps), abs(max(data['A'])-min(data['A'])), bottom=min(data['A']), width=20, color='k')\n",
    "         plt.show()\n",
    "         plt.clf()\n",
    "\n",
    "   def fancy_plot(self, save=False, highlight=False, img_type='png'):\n",
    "      fig = plt.figure(constrained_layout=True)\n",
    "      gs = GridSpec(4,2, figure=fig,hspace=0, wspace=0)\n",
    "      cmap='viridis'\n",
    "      components = []\n",
    "      components.append(fig.add_subplot(gs[3:4, :1]))\n",
    "      components.append(fig.add_subplot(gs[2:3, :1]))\n",
    "      components.append(fig.add_subplot(gs[3:4, -1:]))\n",
    "      components.append(fig.add_subplot(gs[2:3, -1:]))\n",
    "\n",
    "      ax1 = fig.add_subplot(gs[:2,:])\n",
    "      infos = [[\"Mech\"],[\"Scaling\"],[\"Flipped\"],[\"Reversible\"]]\n",
    "      if highlight:\n",
    "         colors=['rebeccapurple', 'dodgerblue', 'cyan', 'black']\n",
    "      else:\n",
    "         colors=['white']*self.num_to_merge\n",
    "      bar_xs = []\n",
    "      bar_widths = []\n",
    "      bounding_boxes = []\n",
    "      bb_colors = ['grey', 'black', 'grey', 'black']\n",
    "      for i, mech in enumerate(self.mechs):\n",
    "         infos[0].append(mech.mech)\n",
    "         infos[1].append(mech.scaling)\n",
    "         infos[2].append(\"yes\" if mech.cathodic else \"no\")\n",
    "         infos[3].append(mech.reversible)\n",
    "         ax = components.pop(0)\n",
    "         ax.get_xaxis().set_ticks([])\n",
    "         ax.get_yaxis().set_ticks([])\n",
    "         ax.patch.set_facecolor(colors[i])\n",
    "         ax.patch.set_alpha(0.2)\n",
    "         ax.scatter(mech.data['V'], mech.data['A'], s=0.02, c=mech.data['v'], cmap=cmap, zorder=1)\n",
    "         min_V = max(0, min(mech.data['V']))\n",
    "         max_V = min(max(self.res['V']), max(mech.data['V']))\n",
    "         bar_width = max_V-min_V\n",
    "         bar_xs.append(min_V + bar_width//2)\n",
    "         bar_widths.append(max_V-min_V)\n",
    "         y_range = max(mech.data['A']) - min(mech.data['A'])\n",
    "         x_range = max(mech.data['V']) - min(mech.data['V'])\n",
    "         n = len(mech.bounding_box)\n",
    "         bounding_boxes.append(mech.bounding_box)\n",
    "         widths = [0.02*x_range]*n\n",
    "         ax.bar(mech.bounding_box, [y_range]*n, bottom=[min(mech.data['A'])]*n, width=widths, zorder=0, color=bb_colors[i])\n",
    "\n",
    "\n",
    "      bottom = min(self.res['A'])\n",
    "      y_range = max(self.res['A']) - min(self.res['A'])\n",
    "      x_range = max(self.res['V']) - min(self.res['V'])\n",
    "      bar_heights = [y_range/self.num_to_merge]*self.num_to_merge\n",
    "      bottoms = [bottom + i*bar_heights[0] for i in range(self.num_to_merge)]\n",
    "      bar_heights = [y_range/self.num_to_merge]*self.num_to_merge\n",
    "      ax1.bar(bar_xs, bar_heights, bottom=bottoms, width=bar_widths, color=colors, alpha=0.2, zorder=0)\n",
    "      ax1.scatter(self.res['V'], self.res['A'], s=0.05, c=self.res['v'], cmap=cmap, zorder=len(bounding_boxes)+1)\n",
    "      for i, bb in enumerate(bounding_boxes):\n",
    "         ax1.bar(bb, y_range, width=0.005*x_range, bottom=min(self.res['A']), color=bb_colors[i], zorder=i+1)\n",
    "      ax1.table(infos, loc='top', cellLoc='center')\n",
    "      ax1.tick_params(axis=\"both\", labelsize=8)\n",
    "      if save:\n",
    "         plt.savefig(f'./{self.date}/Graphs/{self.name}image.{img_type}', dpi=200)\n",
    "      else:\n",
    "         plt.show()\n",
    "      plt.clf()\n",
    "      plt.close('all')\n",
    "\n",
    "   def save(self, dir):\n",
    "      if not os.path.isdir(f'./{self.date}/Generated_Data/{dir}/'):\n",
    "         os.mkdir(f'./{self.date}/Generated_Data/{dir}/')\n",
    "      pickle.dump(self.res, open(f'./{self.date}/Generated_Data/{dir}/{self.name}data', 'wb'))\n",
    "\n",
    "   def get_report(self, file, alias=None):\n",
    "      mech = file[:file.find('_')] if not alias else alias\n",
    "      mech_obj = Mech(mech, False)\n",
    "      mech_obj.file = file\n",
    "      mech_obj.data = pd.read_csv(f'./Samples/{mech}/{file}')\n",
    "      mech_obj.data.loc[:, 'A'] *= mech_obj.scaling\n",
    "      self.mechs = [mech_obj]\n",
    "      mech_obj.reversible = self.__get_reversibility(mech_obj)\n",
    "      self.__calculate_merge_reps(mech_obj)\n",
    "      merge_points = mech_obj.data.loc[mech_obj.data['merge_point']].index\n",
    "      pots = sorted([mech_obj.data.loc[i, 'V'] for i in merge_points])\n",
    "      half_safe = pickle.load(open(f'./Merge_Safety/{mech}_half_safe.pkl', 'rb'))\n",
    "      safe = pickle.load(open(f'./Merge_Safety/{mech}_safe.pkl', 'rb'))\n",
    "      if file in half_safe:\n",
    "         print(\"Half Safe\")\n",
    "      if file in safe:\n",
    "         print(\"Safe\")\n",
    "      self.simple_plot(pots)\n",
    "\n",
    "class RunMergeCV: # ~80/min\n",
    "    def __init__(self, num_runs=1, graph_frequency=5000, report_frequency=5000, save=True, exclude=[]):\n",
    "      self.num_runs = num_runs\n",
    "      self.graph_frequency=graph_frequency\n",
    "      self.report_frequency=report_frequency\n",
    "      self.columns = ['File', 'Mechanism', 'Merge_Left', 'Merge_Right', 'Scan_Rate_Range', 'Min_X', 'Max_X', 'Scaling', 'Reversible', 'Flipped']\n",
    "      self.save = save\n",
    "      self.exclude = exclude\n",
    "      today = datetime.date.today()\n",
    "      self.date = today.strftime(\"%Y%m%d\")\n",
    "\n",
    "    def pretreat(self):\n",
    "      treater = DataPretreatment()\n",
    "      treater.label_merge_safety()\n",
    "      treater.gen_scan_rate_data()\n",
    "      treater.clean_up()\n",
    "\n",
    "    def __add_to_report(self, merger):\n",
    "      all_info = []\n",
    "      sr_range = math.log10(max(merger.res['v']))-math.log10(min(merger.res['v']))\n",
    "      for mech in merger.mechs:\n",
    "         min_V = max(min(merger.res['V']), min(mech.data['V']))\n",
    "         max_V = min(max(merger.res['V']), max(mech.data['V']))\n",
    "         all_info.append([merger.name, mech.mech, mech.bounding_box[0], mech.bounding_box[1], sr_range, min_V, max_V, mech.scaling, mech.reversible, mech.cathodic])\n",
    "      return all_info\n",
    "\n",
    "    def execute(self):\n",
    "      report_info = []\n",
    "      counts = {}\n",
    "      errs = []\n",
    "      dir = self.date + '_' + str(self.report_frequency)\n",
    "      for i in tqdm(range(self.num_runs)):\n",
    "         try:\n",
    "            num_to_merge = r.randint(1,4)\n",
    "            merger = MergeCV(self.date, num_to_merge=num_to_merge, scale_min=0.5, additional_shift_min=100, exclude=self.exclude)\n",
    "            merger.initialize_data()\n",
    "            merger.format_data()\n",
    "            merger.merge()\n",
    "            report_info.extend(self.__add_to_report(merger))\n",
    "            for mech in merger.mechs:\n",
    "               counts[mech.mech] = 1 + counts.get(mech.mech, 0)\n",
    "            if i % self.graph_frequency == 0 or i == self.num_runs-1:\n",
    "               merger.fancy_plot(save=self.save, highlight=True, img_type='jpeg')\n",
    "            if self.save:\n",
    "               merger.save(dir)\n",
    "         except Exception as e:\n",
    "            errs.append((merger.name, e))\n",
    "         if i > 0 and (i % self.report_frequency == 0 or i == self.num_runs-1):\n",
    "            report = pd.DataFrame(report_info, columns=self.columns)\n",
    "            report.to_csv(f'./{self.date}/Reports/info_{self.date}_{i}.csv', index=False)\n",
    "            pickle.dump(counts, open(f'./{self.date}/Reports/counts_{self.date}_{i}', 'wb'))\n",
    "            counts = {}\n",
    "            report_info = []\n",
    "            dir = self.date + '_' + str(i + self.report_frequency)\n",
    "      pickle.dump(errs, open(f'./{self.date}/errs.pkl', 'wb'))\n",
    "\n",
    "date = datetime.date.today()\n",
    "date = str(date).replace('-', '')\n",
    "subfolders = ['Generated_Data', 'Graphs', 'Reports']\n",
    "if not os.path.isdir(f'./{date}'):\n",
    "    os.mkdir(f'./{date}')\n",
    "    for sub in subfolders:\n",
    "        os.mkdir(f'./{date}/{sub}')\n",
    "experiment = RunMergeCV(num_runs=80000, report_frequency=5000, graph_frequency=1000, save=True, exclude=['DISP', 'ECE'])\n",
    "#experiment.pretreat()\n",
    "experiment.execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SR_0.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4660it [16:34,  4.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V</th>\n",
       "      <th>v</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1097.0</td>\n",
       "      <td>3.413253</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1096.0</td>\n",
       "      <td>3.413253</td>\n",
       "      <td>-5.331617e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1095.0</td>\n",
       "      <td>3.413253</td>\n",
       "      <td>-1.066323e-90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1094.0</td>\n",
       "      <td>3.413253</td>\n",
       "      <td>-1.599485e-90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1093.0</td>\n",
       "      <td>3.413253</td>\n",
       "      <td>-7.997426e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1093.0</td>\n",
       "      <td>5.404036</td>\n",
       "      <td>6.336514e-92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1094.0</td>\n",
       "      <td>5.404036</td>\n",
       "      <td>1.267303e-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1095.0</td>\n",
       "      <td>5.404036</td>\n",
       "      <td>8.448685e-92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1096.0</td>\n",
       "      <td>5.404036</td>\n",
       "      <td>4.224343e-92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1097.0</td>\n",
       "      <td>5.404036</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V         v             A\n",
       "0  -1097.0  3.413253  0.000000e+00\n",
       "1  -1096.0  3.413253 -5.331617e-91\n",
       "2  -1095.0  3.413253 -1.066323e-90\n",
       "3  -1094.0  3.413253 -1.599485e-90\n",
       "4  -1093.0  3.413253 -7.997426e-91\n",
       "..     ...       ...           ...\n",
       "4  -1093.0  5.404036  6.336514e-92\n",
       "3  -1094.0  5.404036  1.267303e-91\n",
       "2  -1095.0  5.404036  8.448685e-92\n",
       "1  -1096.0  5.404036  4.224343e-92\n",
       "0  -1097.0  5.404036  0.000000e+00\n",
       "\n",
       "[26340 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SCRIPT FOR FIXING SR CASES\n",
    "\n",
    "import pandas as pd, os, random as r\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = './Samples/SR'\n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if f.endswith('txt')]\n",
    "\n",
    "def fill_scan(df, sr):\n",
    "   if df.loc[df.index[0], 'V'] > df.loc[df.index[-1],'V']:\n",
    "      df = df[::-1]\n",
    "   sr = df['v'].unique()[0]\n",
    "   min_V = int(min(df['V']))\n",
    "   max_V = int(max(df['V']))\n",
    "   og_pots = df['V'].unique()\n",
    "   new_pots = range(min_V, max_V)\n",
    "   res = pd.DataFrame()\n",
    "   res['V'] = new_pots\n",
    "   res['v'] = sr\n",
    "   res = pd.concat([res, df]).groupby(['V','v']).sum().reset_index()\n",
    "   res.loc[~res['V'].isin(og_pots), 'A'] = None\n",
    "   res.interpolate(columns=['A'], inplace=True)\n",
    "   return res\n",
    "\n",
    "for i, file in tqdm(enumerate(files)):\n",
    "   if i == 0:\n",
    "      print(file)\n",
    "   f = pd.read_csv(f'{path}/{file}')\n",
    "   f.columns = ['V', 'A', 'v']\n",
    "   f.loc[:,'V'] *= (20 + 10*r.random())\n",
    "   f.loc[:, 'V'] = round(f.loc[:,'V'])\n",
    "   res = pd.DataFrame()\n",
    "   for v in f['v'].unique():\n",
    "      t = f.loc[f['v'] == v].copy()\n",
    "      ind = t.loc[t['V'] == max(t['V'])].index\n",
    "      fwd = t.loc[:ind[1]]\n",
    "      fwd = fill_scan(fwd, v)\n",
    "      rev = t.loc[ind[1]:]\n",
    "      rev = fill_scan(rev, v)\n",
    "      rev = rev[::-1]\n",
    "      interped = pd.concat([fwd, rev])\n",
    "      res = pd.concat([res, interped])\n",
    "   max_i = max(max(res['A']), abs(min(res['A'])))\n",
    "   res.loc[:, 'A'] /= max_i\n",
    "   res.to_csv(f'{path}/{file}', index=False)\n",
    "   \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SCRIPT FOR MANUAL FILE REMOVAL\n",
    "import pandas as pd, os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import shutil\n",
    "import random as r\n",
    "\n",
    "mech = 'ECP'\n",
    "path = f'./Samples/{mech}'\n",
    "del_path = f'./Deleted_Case_DBs/{mech}'\n",
    "trash_path = f'./TRASH/{mech}_ScanRateFlips'\n",
    "files = pickle.load(open(f'{del_path}/{mech}_to_check', 'rb')) if os.path.isfile(f'{del_path}/{mech}_to_check') else [f for f in os.listdir(path) if f.endswith('txt')]\n",
    "files.sort()\n",
    "to_del = []\n",
    "\n",
    "def plot_me(f, meta_file, path):\n",
    "   for v in f['v'].unique():\n",
    "      t = f.loc[f['v'] == v].copy()\n",
    "      plt.plot(t['V'], t['A'], label=v)\n",
    "   \n",
    "   # print(f'{path}/{meta_file}')\n",
    "   # meta_data = pd.read_csv(f'{path}/{meta_file}')\n",
    "   # labels = ['kfc', 'c_sub', 'c_bulk','i0']\n",
    "   # vals = [round(meta_data.loc[0,label],3) for label in labels]\n",
    "   # info = [labels, vals]\n",
    "   # plt.table(info, loc='top', cellLoc='center')\n",
    "   plt.legend()\n",
    "   plt.show()\n",
    "   plt.clf()\n",
    "\n",
    "def delete(data_path, del_path, trash_path):\n",
    "   for file in os.listdir(del_path):\n",
    "      if file.endswith('check'):\n",
    "         continue\n",
    "      to_del = pickle.load(open(f'{del_path}/{file}', 'rb'))\n",
    "      for case in to_del:\n",
    "         if os.path.isfile(f'{data_path}/{case}'):\n",
    "            shutil.move(f'{data_path}/{case}', f'{trash_path}/{case}')\n",
    "            shutil.move(f'{data_path}/{case[:-4]}', f'{trash_path}/{case[:-4]}')\n",
    "\n",
    "prev = None\n",
    "count = 0\n",
    "stop = False\n",
    "print(len(files))\n",
    "while files and not stop:\n",
    "   file = files.pop(0)\n",
    "   f = pd.read_csv(f'{path}/{file}')\n",
    "   plot_me(f, file[:-4], path)\n",
    "   mark = input()\n",
    "   if mark == 'x':\n",
    "      files.insert(0, file)\n",
    "      files.insert(0, prev)\n",
    "      continue\n",
    "   elif mark == 'd':\n",
    "      to_del.append(file)\n",
    "   elif mark == 'STOP':\n",
    "      stop = True\n",
    "      files.insert(0, file)\n",
    "      pickle.dump(files, open(f'{del_path}/{mech}_to_check', 'wb'))\n",
    "   if count % 100 == 0 or not files or stop:\n",
    "      overlap_preventer = round(1000*r.random())\n",
    "      pickle.dump(to_del,\n",
    "       open(f'{del_path}/{mech}_{overlap_preventer}_{count}', 'wb'))\n",
    "      to_del = []\n",
    "   clear_output()\n",
    "   prev = file\n",
    "   count += 1\n",
    "print(len(files))\n",
    "delete(path, del_path, trash_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT FOR EC1 AND CE FILE TRANSFERS\n",
    "\n",
    "import os, pandas as pd, shutil\n",
    "from tqdm import tqdm\n",
    "path = './Samples'\n",
    "ce_files = [f for f in os.listdir(f'{path}/CE') if f.endswith('.txt')]\n",
    "ec_files = [f for f in os.listdir(f'{path}/EC1') if f.endswith('.txt')]\n",
    "\n",
    "for i, file in tqdm(enumerate(ce_files)):\n",
    "    eca_path = './Samples/ECa'\n",
    "    ecb_path = './Samples/ECb'\n",
    "    new_name = f'CE_{i}'\n",
    "    shutil.copy(f'{path}/CE/{file}', f'{eca_path}/{new_name}.txt')\n",
    "    shutil.copy(f'{path}/CE/{file[:-4]}', f'{eca_path}/{new_name}')\n",
    "    shutil.copy(f'{path}/CE/{file}', f'{ecb_path}/{new_name}.txt')\n",
    "    shutil.copy(f'{path}/CE/{file[:-4]}', f'{ecb_path}/{new_name}')\n",
    "\n",
    "for i, file in tqdm(enumerate(ec_files)):\n",
    "    eca_path = './Samples/ECa'\n",
    "    ecb_path = './Samples/ECb'\n",
    "    new_name = f'EC_{i}'\n",
    "    shutil.copy(f'{path}/EC1/{file}', f'{eca_path}/{new_name}.txt')\n",
    "    shutil.copy(f'{path}/EC1/{file[:-4]}', f'{eca_path}/{new_name}')\n",
    "    shutil.copy(f'{path}/EC1/{file}', f'{ecb_path}/{new_name}.txt')\n",
    "    shutil.copy(f'{path}/EC1/{file[:-4]}', f'{ecb_path}/{new_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('./Reports/') if f.startswith('counts')]\n",
    "res = {}\n",
    "for file in files:\n",
    "    counts = pickle.load(open(f'./Reports/{file}','rb'))\n",
    "    for k, v in counts.items():\n",
    "        res[k] = v + res.get(k, 0)\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e433be03db212ca7e66ccbd162d12c14ec0e9dd2dbe441b2a80f7ef28936c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
